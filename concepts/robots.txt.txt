Robots.txt
==========

Robots.txt is a config format created in 1994 by Martijn Koster.
 https://pldb.io/concepts/../lists/explorer.html#searchBuilder=%7B%22criteria%22%3A%5B%7B%22condition%22%3A%22%3D%22%2C%22data%22%3A%22appeared%22%2C%22origData%22%3A%22appeared%22%2C%22tags%22%3A%22num%22%2C%22value%22%3A%5B%221994%22%5D%7D%5D%2C%22logic%22%3A%22AND%22%7D 1994
 https://pldb.io/concepts/../lists/creators.html#q=Martijn%20Koster Martijn Koster

#2306 on PLDB
30 Years Old

A robots.txt file tells search engine crawlers which URLs the crawler can access on your site.

- Tags: configFormat
- Early development of Robots.txt happened in https://web.archive.org/web/20131029200350/http://inkdroid.org/tmp/www-talk/4113.html
- Read more about Robots.txt on the web: 1. 2.
 https://developers.google.com/search/docs/advanced/robots/intro 1.
 https://en.wikipedia.org/wiki/Robots_exclusion_standard 2.

User-agent: googlebot        # all Google services
Disallow: /private/          # disallow this directory

User-agent: googlebot-news   # only the news service
Disallow: /                  # disallow everything

User-agent: *                # any robot
Disallow: /something/        # disallow this directory

Language features
======================================================

row
 Feature Comments
 FeatureLink ../features/hasComments.html
 Supported ✓
 Example
  # A comment
 Token 
row
 Feature Line Comments
 FeatureLink ../features/hasLineComments.html
 Supported ✓
 Example
  # A comment
 Token #

View source
